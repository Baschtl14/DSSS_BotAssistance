from flask import Flask, request, jsonify
from transformers import AutoModelForCausalLM, AutoTokenizer

app = Flask(__name__)

# Load the TinyLlama model
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

@app.route('/infer', methods=['POST'])
def infer():
    data = request.json
    input_text = data['text']

    # Generate AI response
    inputs = tokenizer(input_text, return_tensors="pt")
    output = model.generate(**inputs, max_length=100)
    response_text = tokenizer.decode(output[0], skip_special_tokens=True)

    return jsonify({'response': response_text})

# Start the Flask server in Colab
app.run(port=5000)

